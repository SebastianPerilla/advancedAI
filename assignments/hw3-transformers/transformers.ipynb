{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ab4eb2c",
   "metadata": {},
   "source": [
    "# Assignment III - Exploring Transformer models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c990ec",
   "metadata": {},
   "source": [
    "## 1. Setting up the Environment:\n",
    "- All relevant libraries are in the requirements.txt\n",
    "- Downloaded the `openai-community/gpt2` model from Hugging Face\n",
    "    - Using the `tokenizer` and `model` instead of the `pipeline` from hugging face "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9455b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
    "import torch\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    \"openai-community/gpt2\",\n",
    "    output_attentions=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e4b1bc",
   "metadata": {},
   "source": [
    "## 2. Understanding the GPT-2 Model:\n",
    "- Investigation of the architecture of the GPT-2 model.\n",
    "    - Components: \n",
    "        1. \n",
    "    - Layers:\n",
    "        1. Yo \n",
    "    - Parameters:\n",
    "        1. yo\n",
    "\n",
    "- Apart from the GPT model, the Hugging Face Transformers library has other model architectures such as BERT, BART or T5, how do the model architectures differ? \n",
    "    - How are the attention mechanisms implemented in these cases?\n",
    "    - What are the advantages and disadvantages?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc3e4a4",
   "metadata": {},
   "source": [
    "## 3. Generating Data through Diverse Strategies + 4. Analysis and evaluation:\n",
    "- Using the **_Hugging Face Documentation_** we can see that we will be using some of the following parameters to experiment with the desired outputs:\n",
    "    - `Temperature` control: *Experiment with different temperature values to control the randomness of generated text.*\n",
    "    - `Top-k` sampling: *Generate text by selecting from the top-k most likely tokens.*\n",
    "    - `Top-p` sampling (nucleus sampling): *Generate text by selecting from the top-p (nucleus) probability distribution.*\n",
    " \n",
    "- [Link to the Documentation](https://huggingface.co/docs/transformers/en/main_classes/text_generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4e075f",
   "metadata": {},
   "source": [
    "### Prompt\n",
    "- We will use the defaults of all of the above mentioned parameters to observe the differences in observed outputs from GPT-2\n",
    "- Here I made a quick helper function to help me better manipulate the parameters and loop through potential outputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4a2509cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Sebastian is\"\n",
    "encoded_prompt = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Helper Function to Run Generation\n",
    "def runGPT(num_output:int = 10, max_length:int = 15, temperature: float = 1.0, top_k: int = 50, top_p: float = 1.0):\n",
    "    # Generating the results with default values in each parameter\n",
    "    for i in range(num_output):\n",
    "        # Generate text using the model\n",
    "        output_sequences = model.generate(\n",
    "            attention_mask=encoded_prompt[\"attention_mask\"],    # This part of the allows for the use of the attention mask with helps the model prioritize certain parts of the generated text, and is most beneficial when applying it alongside padding)\n",
    "            pad_token_id=tokenizer.eos_token_id,                # The id of the <pad> tokens (which helps GPT-2 recognize which tokens are special tokens)\n",
    "            input_ids=encoded_prompt[\"input_ids\"],              # The text used in the prompt for generation \n",
    "            max_length=max_length,                              # Maximum length of the generated text\n",
    "            temperature=temperature,                            # Using the default of 1.0: Controls the randomness of the text (the distribution do_sample takes from higher = more diverse outputs, lower = more deterministic)\n",
    "            top_k=top_k,                                        # Using the default of 50\n",
    "            top_p=top_p,                                        # Using the default of 1.0\n",
    "            do_sample=True,                                     # Enables the model to randomly from a given distribution, other than just taking from the most likely output (creative vs deterministic outputs) \n",
    "        )\n",
    "\n",
    "        generated_text = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
    "        print(generated_text) # Printing the Generated Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4c3e36",
   "metadata": {},
   "source": [
    "### Default Prompt Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "105aecdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sebastian is also on track to secure the fifth consecutive World Cup, and says there are plans to expand into South\n",
      "Sebastian is already showing him his own form under the tutelage of Michael Jackson. But the other big stars\n",
      "Sebastian is an absolute hero, not only in our country, but in the world of business and politics as well\n",
      "Sebastian is a player that is trying to keep pace with everyone else in Barcelona. It is one of the things\n",
      "Sebastian is one of the most highly rated players we have seen. He is playing extremely well. I would certainly\n"
     ]
    }
   ],
   "source": [
    "# Default Prompt Results\n",
    "\n",
    "runGPT(num_output=5, max_length=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bdbe57",
   "metadata": {},
   "source": [
    "#### `Baseline (Default Arguments): temperature=1.0, top_k=50, top_p=1.0`\n",
    "\n",
    "##### Analysis:\n",
    "There is understandable and *grammatically* correct sentences\n",
    "Consistent thoughts througout the sentence, where we can see the following example has text in the beginning and end that relates to itself: \n",
    "- `Sebastian is one of the most highly rated players we have seen. He is playing extremely well. I would certainly` \n",
    "\n",
    "Doesn't deviate too much from the idea aof a \"person\", and recognizes Sebastian as a *central part* of the text it generates.\n",
    "\n",
    "##### Strengths\n",
    "- Meant to be the benchmark, and as expected has a good balance\n",
    "- Outputs are potentially usable for some applications \n",
    "- No obvious errors here and is just generic\n",
    "\n",
    "##### Weaknesses\n",
    "- Very normal and could potentially result in '*limited*' creativity \n",
    "- Some repetitive patterns in style (“Sebastian is [role] who…”), instead of using the text to generate things such as (\"The world champion *Sebastian is* from Brazil\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d0adce",
   "metadata": {},
   "source": [
    "### Temperature Control\n",
    "- The value used to module the next token probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e0db5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sebastian is toying himself just not in that role where he often loses it.\" It was clear with how long\n",
      "Sebastian is always very interesting!\" – Paul McCartney\n",
      "Letting down as he goes on, he will look even\n",
      "Sebastian is working again to bring about a resolution in case other issues continue during our scheduled workshop as outlined previously in\n",
      "Sebastian is to some very poor, vulnerable and sometimes dead victims. The world faces an increasingly existential human threat --\n",
      "Sebastian is a real strength here. With their 4.50 average rating through his third year on Toronto shores where\n"
     ]
    }
   ],
   "source": [
    "# Temperature Control\n",
    "\n",
    "runGPT(num_output=5, max_length=25, temperature=2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b36c9f2",
   "metadata": {},
   "source": [
    "#### `High Temperature (Default at 1.0): temperature=2.0`\n",
    "\n",
    "##### Analysis:\n",
    "Erratic and Creative Outputs from the model\n",
    "Mostly grammatically correct, however at times there are: \n",
    "- Broken and unclear relationships between some of the code generated for example:\n",
    "    - `Sebastian is to some very poor, vulnerable and sometimes dead victims. The world faces an increasingly existential human threat --`\n",
    "- Some phrases feel bizarre or off (“toying himself”, “sometimes dead victims”)\n",
    "\n",
    "##### Strengths\n",
    "- Higher diversity of outputs and more *creative* than the default parameters\n",
    "- Is generating unusual and unexpected ideas compared to the default\n",
    "\n",
    "##### Weaknesses\n",
    "- Significantly less choerent/reliable\n",
    "- Worse phrasing of generated text/unclear text \n",
    "- Results in less *usable* outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1818cd2",
   "metadata": {},
   "source": [
    "### Top-K Sampling\n",
    "- The number of highest probability vocabulary tokens to keep for top-k-filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82d298b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sebastian is already considering a comeback. Many believe there's another, more promising player, in the Brazilian midfielder.\n",
      "Sebastian is a member of the Hildegardner family. Sebastian was born to John and\n",
      "Sebastian is an American, British, Israeli and Palestinian who lives in a small village of about 150 people southeast of\n",
      "Sebastian is one of our biggest personalities, he spends a lot of time with his girls, and at times even\n",
      "Sebastian is so far down the other side of the spectrum that as the man has continued to come out of his\n"
     ]
    }
   ],
   "source": [
    "# Top-K Sampling\n",
    "\n",
    "runGPT(num_output=5, max_length=25, top_k=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6588c1",
   "metadata": {},
   "source": [
    "##### `High Top-K (Default at 50): top_k=100`\n",
    "\n",
    "Analysis:\n",
    "- Valid generated text that doesnt go to far like the temperature parameter\n",
    "- A bit more variety and weirdness in facts (e.g., four nationalities, odd biography)\n",
    "- Sentences remain quite readable although some of them are a bit generic as well\n",
    "\n",
    "Strengths\n",
    "- More lexical and conceptual diversity than the default parameter \n",
    "- Preserves overall coherence better than temperature=2.0\n",
    "\n",
    "Weaknesses\n",
    "- Sometimes drifts into implausible \"hard to believe\" details such as the implausability of one person being:\n",
    "    - `an American, British, Israeli and Palestinian`\n",
    "- Slight increase in “hallucinated” weirdness creating an \"improbable\" backgrounds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf108e7d",
   "metadata": {},
   "source": [
    "### Top-P Sampling (Between 0 - 1 : Default at 1.0)\n",
    "- Only the smallest set of most probable tokens with probabilities that add up to top_p or higher are kept for generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "55f9f469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sebastian is a very smart guy, and he knows what he's doing. He knows how to win, and\n",
      "Sebastian is a member of the \"Greater Than Nothing\" group, a group that seeks to develop a global\n",
      "Sebastian is not the only one who is worried about the situation. He has also expressed his concerns about the future\n",
      "Sebastian is a man who has never been to the World Cup, and he's not even a World Cup winner\n",
      "Sebastian is a big fan of the original and likes to show off his latest creations. He also has a knack\n"
     ]
    }
   ],
   "source": [
    "# Top-P Sampling\n",
    "\n",
    "runGPT(num_output=5, max_length=25, top_p= 0.50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d394b82",
   "metadata": {},
   "source": [
    "#### `High Top-P (Default of 1.0): top_p=0.50 `\n",
    "\n",
    "About Top-P: Keeps the smallest set of most probable tokens, where those probabilities add up to top_p, so the smaller it is, the more \"conservative\" the results will be compared to those at the tail end of the given distribution.\n",
    "\n",
    "Analysis:\n",
    "- All sentences are very coherent and grammatical as we have \"cut\" the tails of the distribution to generate \"less likely\" text.\n",
    "- Outputs stay on a narrow band of plausible, but generic facts.\n",
    "- Almost opposite to the `temperature=2.0` parameter with \"less\" creative text.\n",
    "\n",
    "Strengths\n",
    "- High coherence within the text, where every line reads almost as a believeable fact \n",
    "- Stays on topic, where *Sebastian* is the consistent character and there is no bizarre semantic drift unlike with `temperature`\n",
    "- Controlled text: Seems to output more reliable outputs\n",
    "\n",
    "Weaknesses\n",
    "- Less diversity as the less probable tokens to choose from are no longer an option and therefore text will start to seem \"mundane\" \n",
    "- Lose of some of the creative side that was obtained with higher temperature or wider sampling (top_k=100 or top_p≈1.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9121599a",
   "metadata": {},
   "source": [
    "## 5. Deep-dive into attention scores:\n",
    "- Analyze attention scores produced by the GPT-2 model. What do these scores represent, and how can they be interpreted?\n",
    "- Visualize attention matrices for specific input sequences. Observe how attention is distributed across tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e76bd4f",
   "metadata": {},
   "source": [
    "#### Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f2f05f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ff7cf2f",
   "metadata": {},
   "source": [
    "#### Visualization of Attention Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d06958d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64437dcb",
   "metadata": {},
   "source": [
    "### 6. [Extra] Fine-tune GPT2 model\n",
    "- Using the movie scripts dataset available in Kaggle (or any other one dataset of your choice). Fine-tune the GPT model for a couple of epochs. Include a prompt and evaluate how the text generated from a fine-tuned model differs from the previous text generation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
